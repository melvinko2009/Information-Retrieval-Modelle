{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import fasttext\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Daten, Testanfragen und Modelle laden\n",
    "with open(\"../../Data/Data.json\",encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(\"../../Data/Queries.json\",encoding='utf-8') as file:\n",
    "    queries = json.load(file)\n",
    "    \n",
    "#Deutsche Stopwörter und Lemmatizer laden\n",
    "german_stopwords = stopwords.words('german')\n",
    "lemmatizer = spacy.load('de_core_news_sm')\n",
    "\n",
    "#Word2Vec Modell laden\n",
    "w2vModel = fasttext.load_model('../../Data/Word2Vec/cc.de.300.bin')\n",
    "w2vvocab = w2vModel.get_words()\n",
    "\n",
    "#Vorhersagen abspeichern\n",
    "predictions = dict()\n",
    "\n",
    "# Hyperparameter\n",
    "DROPOUT_RATE, BATCH_SIZE, EPOCHS = 0.5, 32, 10\n",
    "LOSS_FUNCTION = 'binary_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "ACTIVATION = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing der PDF-Texte\n",
    "def preprocessing(text):\n",
    "    \n",
    "    #Kleinbuchstaben & einzelne Zeichen entfernen & mehrere Leerzeichen entfernen\n",
    "    text = cleanData(text)\n",
    "        \n",
    "    #Stopwörter entfernen und Lemmas erzuegen\n",
    "    text = removeStopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    text = word_tokenize(text, language='german')\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Data Cleaning\n",
    "def cleanData(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub('[^a-zA-ZäöüÄÖÜß]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Stopword Removal\n",
    "def removeStopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in german_stopwords])\n",
    "\n",
    "#Lemmatization\n",
    "def lemmatize(text):\n",
    "    doc = lemmatizer(text)\n",
    "    return ' '.join([x.lemma_ for x in doc]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten bereinigen\n",
    "preprocessedData = dict()\n",
    "for key,value in data.items():\n",
    "    preprocessedData[key] = preprocessing(value['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec laden und trainieren\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in preprocessedData.items()]\n",
    "doc2vecModel = Doc2Vec(documents, vector_size=300,window=5,min_count=1, workers=4,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methoden für VectorSpaceModel\n",
    "def get_Word2Vec_embedding(text):\n",
    "    embedding = []\n",
    "    for token in text:\n",
    "        embedding.append(w2vModel.get_word_vector(token))\n",
    "    return np.mean(embedding, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalierungsmetriken\n",
    "\n",
    "#Precision@k\n",
    "def precision(truth, predicted, k):\n",
    "    summe,count = 0, 0\n",
    "    for i in range(0,k+1):\n",
    "        if int(predicted[i]) in truth:\n",
    "            count+=1\n",
    "    summe+= count/ (i+1)\n",
    "    return summe\n",
    "\n",
    "def average_precision(predictedDocuments,relDocuments):\n",
    "    averageSum = 0\n",
    "    for i in range(0,len(predictedDocuments)):\n",
    "        summe = 0  \n",
    "        if int(predictedDocuments[i]) in relDocuments:\n",
    "            summe+= precision(relDocuments, predictedDocuments, i)\n",
    "        averageSum += summe/len(relDocuments)\n",
    "    return averageSum\n",
    "\n",
    "#RR\n",
    "def reciprocal_rank(predictedDocuments, relDocuments):\n",
    "    for i in range(0, len(predictedDocuments)):\n",
    "        if int(predictedDocuments[i]) in relDocuments:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "#MAP und MRR für das NN berechnen\n",
    "def calculateMetrics(model,insert=True):\n",
    "    evalData = []\n",
    "    mapSum,mrrSum = 0,0\n",
    "    for q in queries:\n",
    "        query, relDocuments = q[0], q[1]\n",
    "        \n",
    "        #InputVektor für die Anfrage bestimmen\n",
    "        queryVector = word_tokenize(query, language='german')\n",
    "        queryVector = get_Word2Vec_embedding(queryVector)\n",
    "        for key in preprocessedData.keys():\n",
    "            evalData.append(np.append(queryVector,doc2vecModel.docvecs[key]))\n",
    "        \n",
    "        #Prediction für alle Dokumente für die aktuelle Anfrage abholen\n",
    "        ypred = model.predict(np.array(evalData))\n",
    "       \n",
    "        #Dictionary mit DokumentId und Score Paaren bilden und absteigend sortieren\n",
    "        docScore = dict()\n",
    "        for i in range(len(ypred)):\n",
    "            docScore[i] = ypred[i][0]\n",
    "            \n",
    "        docScore = dict(sorted(docScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "        predictedDocuments = list(docScore.keys())\n",
    "        \n",
    "        #MAP und MRR um AveragePrecision und RR aufsummieren\n",
    "        ap = average_precision(predictedDocuments,relDocuments)\n",
    "        rr = reciprocal_rank(predictedDocuments,relDocuments)\n",
    "        mapSum+= ap\n",
    "        mrrSum+= rr\n",
    "        \n",
    "        scores = dict()\n",
    "        scores['MAP'],scores['MRR']  = ap,rr\n",
    "        predictions[query] = scores\n",
    "       \n",
    "    #MAP und MRR errechnen\n",
    "    mean_average_precision = mapSum / len(queries)\n",
    "    mean_reciprocal_rank = mrrSum / len(queries)\n",
    "    return mean_average_precision, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Neuronales Netz mit binärer Entscheidung für Relevanz mit Word2Vec Anfrage und Doc2Vec Dokument als Input\n",
    "\n",
    "# Daten in binäre Relevanzzugehörigkeit umwandeln\n",
    "def createBinaryTestData():\n",
    "    inputData, labels = [],[]\n",
    "    for q in queries:\n",
    "        query,relDocuments = q[0],q[1]\n",
    "        query = word_tokenize(query, language='german')\n",
    "        queryVector = get_Word2Vec_embedding(query)\n",
    "        for key,doc in preprocessedData.items():\n",
    "            inputData.append(np.append(queryVector,doc2vecModel.docvecs[key]))\n",
    "            labels.append(int(int(key) in relDocuments))\n",
    "    trainData, testData, trainLabel, testLabel = splitData(inputData,labels)\n",
    "    return trainData, testData, trainLabel, testLabel\n",
    "\n",
    "# Daten in Test-und Trainingsdaten splitten\n",
    "def splitData(x,y):\n",
    "    trainData, testData, trainLabel, testLabel = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "    return np.array(trainData), np.array(testData), np.array(trainLabel),np.array(testLabel)\n",
    "\n",
    "trainData, testData, trainLabel, testLabel = createBinaryTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfaches binäres Perceptron\n",
    "def createPerceptron():\n",
    "    perceptron = Sequential()\n",
    "    perceptron.add(Dense(32, input_dim=len(trainData[0]), activation=ACTIVATION))\n",
    "    perceptron.add(Dropout(DROPOUT_RATE))\n",
    "    perceptron.add(Dense(16, activation='relu'))\n",
    "    perceptron.add(Dropout(DROPOUT_RATE))\n",
    "    perceptron.add(Dense(1, activation='sigmoid'))\n",
    "    perceptron.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER, metrics='accuracy')\n",
    "    return perceptron   \n",
    "\n",
    "# Modell trainieren\n",
    "def trainModel(model,trainX,trainY,testX,testY,verb=2):\n",
    "    history = model.fit(trainX,trainY, verbose=verb,batch_size=BATCH_SIZE ,epochs=EPOCHS,validation_data=(testX, testY))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron-Modell trainieren\n",
    "a1 = datetime.datetime.now()\n",
    "model = createPerceptron()\n",
    "history = trainModel(model,trainData,trainLabel,testData,testLabel)\n",
    "b1 = datetime.datetime.now()\n",
    "modelTime = b1 - a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    a = datetime.datetime.now()\n",
    "    result = calculateMetrics(model)\n",
    "    b = datetime.datetime.now()\n",
    "    print(f'NN | MAP: {str(result[0])}, MRR: {str(result[1])}, Anfragebearbeitung {b-a}, Modellaufbau {modelTime}')\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AllPredictions laden\n",
    "with open(\"../../Data/Predictions.json\",encoding='utf-8':\n",
    "    allPredictions = json.load(file)\n",
    "\n",
    "for query, scores in allPredictions.items():\n",
    "    scores['NN'] = predictions[query]\n",
    "\n",
    "#Predictions abspeichern\n",
    "with open('../../Data/Predictions.json', 'w',encoding='utf-8') as fp:\n",
    "    json.dump(allPredictions, fp,  indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
