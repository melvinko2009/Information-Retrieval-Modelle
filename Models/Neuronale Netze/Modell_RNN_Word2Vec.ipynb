{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "import fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,  Dropout, Embedding, Flatten, LSTM, GRU, CuDNNGRU, CuDNNLSTM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Daten, Testanfragen und Modelle laden\n",
    "with open(\"../../Data/Data.json\",encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(\"../../Data/Queries.json\",encoding='utf-8') as file:\n",
    "    queries = json.load(file)\n",
    "    \n",
    "#Deutsche Stopwörter und Lemmatizer laden\n",
    "german_stopwords = stopwords.words('german')\n",
    "lemmatizer = spacy.load('de_core_news_sm')\n",
    "\n",
    "#Word2Vec Modell laden\n",
    "w2vModel = fasttext.load_model('../../Data/Word2Vec/cc.de.300.bin')\n",
    "w2vvocab = w2vModel.get_words()\n",
    "\n",
    "#Vorhersagen abspeichern\n",
    "predictions = dict()\n",
    "\n",
    "# Hyperparameter\n",
    "DROPOUT_RATE, BATCH_SIZE, EPOCHS = 0.5, 32, 5\n",
    "LOSS_FUNCTION = 'binary_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "ACTIVATION = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing der PDF-Texte\n",
    "def preprocessing(text):\n",
    "    \n",
    "    #Kleinbuchstaben & einzelne Zeichen entfernen & mehrere Leerzeichen entfernen\n",
    "    text = cleanData(text)\n",
    "        \n",
    "    #Stopwörter entfernen und Lemmas erzuegen\n",
    "    text = removeStopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "#Data Cleaning\n",
    "def cleanData(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub('[^a-zA-ZäöüÄÖÜß]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Stopword Removal\n",
    "def removeStopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in german_stopwords])\n",
    "\n",
    "#Lemmatization\n",
    "def lemmatize(text):\n",
    "    doc = lemmatizer(text)\n",
    "    return ' '.join([x.lemma_ for x in doc]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten bereinigen\n",
    "preprocessedData = dict()\n",
    "for key,value in data.items():\n",
    "    preprocessedData[key] = preprocessing(value['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methoden für VectorSpaceModel\n",
    "def get_Word2Vec_embedding(text):\n",
    "    embedding = []\n",
    "    for token in text:\n",
    "        embedding.append(w2vModel.get_word_vector(token))\n",
    "    return np.mean(embedding, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalierungsmetriken\n",
    "\n",
    "#Precision@k\n",
    "def precision(truth, predicted, k):\n",
    "    summe,count = 0, 0\n",
    "    for i in range(0,k+1):\n",
    "        if int(predicted[i]) in truth:\n",
    "            count+=1\n",
    "    summe+= count/ (i+1)\n",
    "    return summe\n",
    "\n",
    "def average_precision(predictedDocuments,relDocuments):\n",
    "    averageSum = 0\n",
    "    for i in range(0,len(predictedDocuments)):\n",
    "        summe = 0  \n",
    "        if int(predictedDocuments[i]) in relDocuments:\n",
    "            summe+= precision(relDocuments, predictedDocuments, i)\n",
    "        averageSum += summe/len(relDocuments)\n",
    "    return averageSum\n",
    "\n",
    "#RR\n",
    "def reciprocal_rank(predictedDocuments, relDocuments):\n",
    "    for i in range(0, len(predictedDocuments)):\n",
    "        if int(predictedDocuments[i]) in relDocuments:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "#MAP und MRR für das NN berechnen\n",
    "def calculateMetrics(model,insert=True):\n",
    "    evalData = []\n",
    "    mapSum,mrrSum = 0,0\n",
    "    for q in queries:\n",
    "        query, relDocuments = q[0], q[1]\n",
    "        \n",
    "        #InputVektor für die Anfrage bestimmen\n",
    "        tempInputData = []\n",
    "        for key,doc in preprocessedData.items():\n",
    "            tempInputData.append(query + \" \" + doc)\n",
    "            \n",
    "        inputTokens = tokenizer.texts_to_sequences(tempInputData)\n",
    "        inputPad = pad_sequences(inputTokens,maxlen=maxLen,padding='post')\n",
    "        \n",
    "        ypred = model.predict(np.array(inputPad))\n",
    "       \n",
    "        #Dictionary mit DokumentId und Score Paaren bilden und absteigend sortieren\n",
    "        docScore = dict()\n",
    "        for i in range(len(ypred)):\n",
    "            docScore[i] = ypred[i][0]\n",
    "            \n",
    "        docScore = dict(sorted(docScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "        predictedDocuments = list(docScore.keys())\n",
    "        \n",
    "        #MAP und MRR um AveragePrecision und RR aufsummieren\n",
    "        ap = average_precision(predictedDocuments,relDocuments)\n",
    "        rr = reciprocal_rank(predictedDocuments,relDocuments)\n",
    "        mapSum+= ap\n",
    "        mrrSum+= rr\n",
    "        \n",
    "        scores = dict()\n",
    "        scores['MAP'],scores['MRR']  = ap,rr\n",
    "        predictions[query] = scores\n",
    "       \n",
    "    #MAP und MRR errechnen\n",
    "    mean_average_precision = mapSum / len(queries)\n",
    "    mean_reciprocal_rank = mrrSum / len(queries)\n",
    "    return mean_average_precision, mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten in binäre Relevanzzugehörigkeit umwandeln\n",
    "def createTestData():\n",
    "    inputData, labels = [],[]\n",
    "    for q in queries:\n",
    "        query,relDocuments = q[0],q[1]\n",
    "        for key,doc in preprocessedData.items():\n",
    "            inputData.append(query + \" \" + doc)\n",
    "            labels.append(int(int(key) in relDocuments))\n",
    "    return inputData, labels\n",
    "\n",
    "# Daten in Test-und Trainingsdaten splitten\n",
    "def splitData(x,y):\n",
    "    trainData, testData, trainLabel, testLabel = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "    return np.array(trainData), np.array(testData), np.array(trainLabel),np.array(testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten in Trainings-und Testdaten splitten\n",
    "inputData, labels = createTestData()\n",
    "trainData, testData, trainLabel, testLabel = splitData(inputData, labels)\n",
    "\n",
    "#Daten in passendes Format umwandeln \n",
    "totalData = np.append(trainData, testData)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(totalData)\n",
    "\n",
    "maxLen = np.max([len(s.split()) for s in totalData])\n",
    "vocabSize = len(tokenizer.word_index) + 1\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(trainData)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(testData)\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens,maxlen=maxLen,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens,maxlen=maxLen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Emedding Matrix\n",
    "embedding_matrix = np.zeros((vocabSize, 300))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = w2vModel.get_word_vector(word) \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN mit GRU initialsieren\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(vocabSize, 300, weights = [embedding_matrix], input_length = maxLen, trainable = False))\n",
    "#model.add(GRU(units=2,dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#RNN mit LSTM initialsieren\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 300, weights = [embedding_matrix], input_length = maxLen, trainable = False))\n",
    "model.add(LSTM(64,input_length=maxLen, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# RNN initialsieren und trainieren\n",
    "a1 = datetime.datetime.now()\n",
    "model.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER, metrics='accuracy')\n",
    "model.fit(x_train_pad,trainLabel, verbose=2,batch_size=BATCH_SIZE ,epochs=EPOCHS,validation_data=(x_test_pad, testLabel))\n",
    "modelTime = datetime.datetime.now() - a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    a = datetime.datetime.now()\n",
    "    result = calculateMetrics(model)\n",
    "    b = datetime.datetime.now()\n",
    "    print(f'GRU | MAP: {str(result[0])}, MRR: {str(result[1])}, Anfragebearbeitung: {b-a}')\n",
    "\n",
    "#RNN evaluieren\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AllPredictions laden\n",
    "with open(\"../../Data/Predictions.json\",encoding='utf-8') as file:\n",
    "    allPredictions = json.load(file)\n",
    "\n",
    "for query, scores in allPredictions.items():\n",
    "    scores['RNN'] = predictions[query]\n",
    "\n",
    "#Predictions abspeichern\n",
    "with open('../../Data/Predictions.json', 'w',encoding='utf-8') as fp:\n",
    "    json.dump(allPredictions, fp,  indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modell abspeichern\n",
    "model.save('SavedModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modell ladenm\n",
    "model = keras.moels.load_model('SavedModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
